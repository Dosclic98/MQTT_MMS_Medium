{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "#import pgmpy.models\n",
    "#import pgmpy.inference\n",
    "import numpy as np\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSlices = 24\n",
    "\n",
    "class NodeNames(Enum):\n",
    "    MITM = \"MITM\"\n",
    "    SRM = \"SRM\"\n",
    "    UC = \"UC\"\n",
    "    UPS = \"UPS\"\n",
    "    IMD = \"IMD\"\n",
    "    MC = \"MC\"\n",
    "    CC = \"CC\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySmile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "#evidenceNodes = [NodeNames.IMD.value, NodeNames.MC.value, NodeNames.CC.value]\n",
    "evidenceNodes = []\n",
    "\n",
    "trainFileName = \"outTrain.csv\"\n",
    "testFileName = \"outTest.csv\"\n",
    "networkFileName = \"DBN-MITM.xdsl\"\n",
    "\n",
    "classNodesHandles = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node_info(net, node_handle):\n",
    "    print(\"Node id/name: \" + net.get_node_id(node_handle) + \"/\" +\n",
    "    net.get_node_name(node_handle))\n",
    "    print(\" Outcomes: \" + \" \".join(net.get_outcome_ids(node_handle)))\n",
    "    parent_ids = net.get_parent_ids(node_handle)\n",
    "    if len(parent_ids) > 0:\n",
    "        print(\" Parents: \" + \" \".join(parent_ids))\n",
    "    child_ids = net.get_child_ids(node_handle)\n",
    "    if len(child_ids) > 0:\n",
    "        print(\" Children: \" + \" \".join(child_ids))\n",
    "    print_cpt_matrix(net, node_handle)\n",
    "    \n",
    "def print_cpt_matrix(net, node_handle):\n",
    "    cpt = net.get_node_definition(node_handle)\n",
    "    parents = net.get_parents(node_handle)\n",
    "    dim_count = 1 + len(parents)\n",
    "    dim_sizes = [0] * dim_count\n",
    "    for i in range(0, dim_count - 1):\n",
    "        dim_sizes[i] = net.get_outcome_count(parents[i])\n",
    "    dim_sizes[len(dim_sizes) - 1] = net.get_outcome_count(node_handle)\n",
    "    coords = [0] * dim_count\n",
    "    for elem_idx in range(0, len(cpt)):\n",
    "        index_to_coords(elem_idx, dim_sizes, coords)\n",
    "        outcome = net.get_outcome_id(node_handle, coords[dim_count - 1])\n",
    "        out_str = \" P(\" + outcome\n",
    "        if dim_count > 1:\n",
    "            out_str += \" | \"\n",
    "            for parent_idx in range(0, len(parents)):\n",
    "                if parent_idx > 0:\n",
    "                    out_str += \",\"\n",
    "                parent_handle = parents[parent_idx]\n",
    "                out_str += net.get_node_id(parent_handle) + \"=\" + \\\n",
    "                net.get_outcome_id(parent_handle, coords[parent_idx])\n",
    "        prob = cpt[elem_idx]\n",
    "        out_str += \")=\" + str(prob)\n",
    "        print(out_str)\n",
    "    \n",
    "        \n",
    "def index_to_coords(index, dim_sizes, coords):\n",
    "    prod = 1\n",
    "    for i in range(len(dim_sizes) - 1, -1, -1):\n",
    "        coords[i] = int(index / prod) % dim_sizes[i]\n",
    "        prod *= dim_sizes[i]\n",
    "\n",
    "def pint_time_cpt_marix(net, nodeHandle):\n",
    "    timeCPT = net.get_node_temporal_definition(nodeHandle, 1)\n",
    "    \n",
    "        \n",
    "def plot_time_CPT(net, nodeHandle):    \n",
    "    cpt = net.get_node_temporal_definition(nodeHandle, 1)\n",
    "    print(len(cpt))\n",
    "    print(\"###\")\n",
    "    \n",
    "def print_net_info(net, unrolled = True):\n",
    "    for n in net.get_all_nodes():\n",
    "        print_node_info(net, n)\n",
    "        if not unrolled and net.get_node_id(n) == NodeNames.UPS.value:\n",
    "            plot_time_CPT(net, n)\n",
    "            \n",
    "def calc_stat(confMatrix, outcome, type='P'):\n",
    "    TP = confMatrix[outcome][outcome]\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(0, len(confMatrix)):\n",
    "        for j in range(0, len(confMatrix[i])):\n",
    "            if i == outcome and j != outcome:\n",
    "                FP += confMatrix[i][j]\n",
    "            if j == outcome and i != outcome:\n",
    "                FN += confMatrix[i][j]\n",
    "            if i != outcome and j != outcome:\n",
    "                TN += confMatrix[i][j] \n",
    "                \n",
    "    if type == 'P':\n",
    "        if TP + FP == 0: return float('nan')\n",
    "        return TP / (TP + FP)\n",
    "    if type == 'A':\n",
    "        if TP + TN + FP + FN == 0: return float('nan')\n",
    "        return  (TP+TN) / (TP + TN + FP + FN)\n",
    "    if type == 'R':\n",
    "        if TP + FN == 0: return float('nan')\n",
    "        return TP / (TP + FN)\n",
    "    if type == 'F':\n",
    "        if (2*TP)+FP+FN == 0: return float('nan')\n",
    "        return (2*TP)/((2*TP)+FP+FN)        \n",
    "\n",
    "def print_validator_results(net, originalSliceCount, validator, nodeId): \n",
    "    nodeHandle = classNodesHandles[nodeId]\n",
    "    outcomeCount = net.get_outcome_count(nodeHandle)\n",
    "    accMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    precMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    recMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    fMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    \n",
    "    \n",
    "    for slice in range(1, originalSliceCount):\n",
    "        if DEBUG: print(\"### Slice \" + str(slice) + \" ###\")\n",
    "        nodeHandle = classNodesHandles[nodeId + \"_\" + str(slice)]\n",
    "        cm = validator.get_confusion_matrix(nodeHandle)\n",
    "        for i in range(0, outcomeCount):\n",
    "            acc = calc_stat(cm, i, 'A')\n",
    "            # If the calculated statistic is NaN assign 0 (see next line)\n",
    "            accMtrx[i][0][slice] = acc if not math.isnan(acc) else 0\n",
    "            # If the calculated statistic is NaN assign weight 0 to it\n",
    "            # otherwise use the number of elements belonging to that class as weight\n",
    "            accMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(acc) else 0\n",
    "            prec = calc_stat(cm, i, 'P')\n",
    "            precMtrx[i][0][slice] = prec if not math.isnan(prec) else 0\n",
    "            precMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(prec) else 0\n",
    "            rec = calc_stat(cm, i, 'R')\n",
    "            recMtrx[i][0][slice] = rec if not math.isnan(rec) else 0\n",
    "            recMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(rec) else 0\n",
    "            f = calc_stat(cm, i, 'F')\n",
    "            fMtrx[i][0][slice] = f if not math.isnan(f) else 0\n",
    "            fMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(f) else 0\n",
    "            \n",
    "            if DEBUG:\n",
    "                print(\"Accuracy for \" + nodeId + str(i) + \": \" + str(acc))\n",
    "                print(\"Precision for \" + nodeId + str(i) + \": \" + str(prec))\n",
    "                print(\"Recall for \" + nodeId + str(i) + \": \" + str(rec))    \n",
    "        if DEBUG:    \n",
    "            print(\"** Confusion Matrix **\")\n",
    "            for i in range(0, outcomeCount):\n",
    "                print(cm[i])\n",
    "            print(\"\")\n",
    "    \n",
    "    # Calculates weighted averages (each statistic of each timeslice is weighted\n",
    "    # by the number of elements nelonging to that class in that timeslice)\n",
    "    avgAccOut = np.zeros(outcomeCount)\n",
    "    avgPrecOut = np.zeros(outcomeCount)\n",
    "    avgRecOut = np.zeros(outcomeCount)\n",
    "    avgFOut = np.zeros(outcomeCount)\n",
    "    for i in range(0, outcomeCount):\n",
    "        avgAccOut[i] = np.sum(accMtrx[i][0]*accMtrx[i][1])/np.sum(accMtrx[i][1])\n",
    "        avgPrecOut[i] = np.sum(precMtrx[i][0]*precMtrx[i][1])/np.sum(precMtrx[i][1])\n",
    "        avgRecOut[i] = np.sum(recMtrx[i][0]*recMtrx[i][1])/np.sum(recMtrx[i][1])\n",
    "        avgFOut[i] = np.sum(fMtrx[i][0]*fMtrx[i][1])/np.sum(fMtrx[i][1])\n",
    "        \n",
    "        \n",
    "        print(\"Average Accuracy for \" + nodeId + str(i) + \": \" + str(avgAccOut[i]))\n",
    "        print(\"Average Precision for \" + nodeId + str(i) + \": \" + str(avgPrecOut[i]))\n",
    "        print(\"Average Recall for \" + nodeId + str(i) + \": \" + str(avgRecOut[i]))\n",
    "        print(\"Average F-score for \" + nodeId + str(i) + \": \" + str(avgFOut[i]))\n",
    "        print(\"\") \n",
    "    \n",
    "    out = {}\n",
    "    out['A'] = np.average(avgAccOut)\n",
    "    out['P'] = np.average(avgPrecOut) \n",
    "    out['R'] = np.average(avgRecOut) \n",
    "    out['F'] = np.average(avgFOut)\n",
    "    return out\n",
    "        \n",
    "            \n",
    "def eraseDefinitions(net):\n",
    "    nodes = net.get_all_node_ids()\n",
    "    for node in nodes:\n",
    "        cpt = net.get_node_definition(node)\n",
    "        numOutcomes = net.get_outcome_count(node)\n",
    "        p = 1 / numOutcomes\n",
    "        for i in range(0, len(cpt)):\n",
    "            cpt[i] = p\n",
    "        net.set_node_definition(node, cpt)\n",
    "\n",
    "def eraseTemporalDefinitions(net):\n",
    "    nodes = net.get_all_node_ids()\n",
    "    for node in nodes:\n",
    "        try:\n",
    "            cpt = net.get_node_temporal_definition(node, 1)\n",
    "            numOutcomes = net.get_outcome_count(node)\n",
    "            p = 1 / numOutcomes\n",
    "            for i in range(0, len(cpt)):\n",
    "                cpt[i] = p\n",
    "            net.set_node_temporal_definition(node, 1, cpt)  \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def partializeEvidence(df, evidenceNodes, sliceCount):\n",
    "    for evidenceNode in evidenceNodes:\n",
    "        for slice in range(1, sliceCount):\n",
    "            if 1 <= slice <= 20:\n",
    "                colName = evidenceNode + \"_\" + str(slice)\n",
    "                df = df.drop(colName, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and read the DBN from file\n",
    "net = pysmile.Network()\n",
    "ds = pysmile.learning.DataSet()\n",
    "\n",
    "net.read_file(os.getcwd() + \"/../../../Genie-DBN/\" + networkFileName)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erase CPTs before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eraseDefinitions(net)\n",
    "eraseTemporalDefinitions(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traininig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-LL: -26735.77419596679\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/\" + trainFileName)\n",
    "ds.read_pandas_dataframe(df)\n",
    "\n",
    "matching = ds.match_network(net)\n",
    "em = pysmile.learning.EM()\n",
    "# Small data variatons correspond to big changes\n",
    "em.set_relevance(0)\n",
    "res = em.learn(ds, net, matching)\n",
    "\n",
    "print(\"N-LL: \" + str(em.get_last_score()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPerf(net, nodeName, testDs = None):\n",
    "    if testDs is None:\n",
    "        testDs = pd.read_csv(os.getcwd() + \"/\" + testFileName)\n",
    "        \n",
    "    ds.read_pandas_dataframe(testDs)\n",
    "    \n",
    "    unrolledNet = net.unroll().unrolled     \n",
    "    matching = ds.match_network(unrolledNet)\n",
    "    validator = pysmile.learning.Validator(unrolledNet, ds, matching)\n",
    "    # Set class nodes (those that will not be considered as evidence nodes)\n",
    "    for elem in NodeNames.__members__:\n",
    "        if elem not in evidenceNodes:\n",
    "            classNodesHandles[elem] = unrolledNet.get_node(elem)\n",
    "            validator.add_class_node(classNodesHandles[elem])\n",
    "            for slice in range(1, net.get_slice_count()):\n",
    "                elemCat = elem + \"_\" + str(slice)\n",
    "                classNodesHandles[elemCat] = unrolledNet.get_node(elemCat)\n",
    "                validator.add_class_node(classNodesHandles[elemCat])\n",
    "    # Test the predctions on the class nodes            \n",
    "    validator.test()\n",
    "    #print(validator.get_result_dataset())\n",
    "    if DEBUG:\n",
    "        print_net_info(unrolledNet, unrolled=True)\n",
    "    \n",
    "    return print_validator_results(unrolledNet, net.get_slice_count(), validator, nodeName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "200\n",
      "Duplicate rows: 1\n"
     ]
    }
   ],
   "source": [
    "def compare_files(trainFileName, testFileName):\n",
    "    dfTrain = pd.read_csv(os.getcwd() + \"/\" + trainFileName)\n",
    "    dfTest = pd.read_csv(os.getcwd() + \"/\" + testFileName)\n",
    "    \n",
    "    dfOut = pd.concat([dfTrain, dfTest]).drop_duplicates(keep=\"first\")\n",
    "    dups  = dfTrain.shape[0] + dfTest.shape[0] - dfOut.shape[0]\n",
    "    print(dfTrain.shape[0])\n",
    "    print(dfTest.shape[0])\n",
    "    print(\"Duplicate rows: \" + str(dups))\n",
    "    \n",
    "compare_files(\"outTrain.csv\", \"outTest.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test learning capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-LL: -23912.13990812209\n",
      "Average Accuracy for UPS0: 0.8845045045045045\n",
      "Average Precision for UPS0: 0.8117117117117117\n",
      "Average Recall for UPS0: 0.9478773584905661\n",
      "Average F-score for UPS0: 0.7882337495300542\n",
      "\n",
      "Average Accuracy for UPS1: 0.8274074074074075\n",
      "Average Precision for UPS1: 0.9271604938271605\n",
      "Average Recall for UPS1: 0.8298768308921437\n",
      "Average F-score for UPS1: 0.8314042092266657\n",
      "\n",
      "N-LL: -23954.393167603354\n",
      "Average Accuracy for UPS0: 0.8986878453038674\n",
      "Average Precision for UPS0: 0.8259668508287292\n",
      "Average Recall for UPS0: 0.9438266443701225\n",
      "Average F-score for UPS0: 0.8003175630064049\n",
      "\n",
      "Average Accuracy for UPS1: 0.8297661870503599\n",
      "Average Precision for UPS1: 0.9244604316546763\n",
      "Average Recall for UPS1: 0.8322146562905318\n",
      "Average F-score for UPS1: 0.8335486508305764\n",
      "\n",
      "N-LL: -24155.805911717332\n",
      "Average Accuracy for UPS0: 0.8898746701846966\n",
      "Average Precision for UPS0: 0.8117854001759015\n",
      "Average Recall for UPS0: 0.9677004333694474\n",
      "Average F-score for UPS0: 0.7973184555617077\n",
      "\n",
      "Average Accuracy for UPS1: 0.8393518518518518\n",
      "Average Precision for UPS1: 0.9527458492975734\n",
      "Average Recall for UPS1: 0.8410187667560323\n",
      "Average F-score for UPS1: 0.8577918615491446\n",
      "\n",
      "N-LL: -24013.034595207384\n",
      "Average Accuracy for UPS0: 0.8785825199645075\n",
      "Average Precision for UPS0: 0.7994676131322094\n",
      "Average Recall for UPS0: 0.9486820199778024\n",
      "Average F-score for UPS0: 0.7765431214194615\n",
      "\n",
      "Average Accuracy for UPS1: 0.8131620428751577\n",
      "Average Precision for UPS1: 0.9255989911727617\n",
      "Average Recall for UPS1: 0.8155313351498638\n",
      "Average F-score for UPS1: 0.8205156085510084\n",
      "\n",
      "N-LL: -24146.162533508577\n",
      "Average Accuracy for UPS0: 0.8921178343949044\n",
      "Average Precision for UPS0: 0.8316651501364877\n",
      "Average Recall for UPS0: 0.9579048140043764\n",
      "Average F-score for UPS0: 0.8126453153918588\n",
      "\n",
      "Average Accuracy for UPS1: 0.8630481120584653\n",
      "Average Precision for UPS1: 0.9439707673568819\n",
      "Average Recall for UPS1: 0.8646290322580645\n",
      "Average F-score for UPS1: 0.8647459047544428\n",
      "\n",
      "N-LL: -24137.140522077327\n",
      "Average Accuracy for UPS0: 0.8876632462686568\n",
      "Average Precision for UPS0: 0.8302238805970149\n",
      "Average Recall for UPS0: 0.936685393258427\n",
      "Average F-score for UPS0: 0.8012926738886702\n",
      "\n",
      "Average Accuracy for UPS1: 0.8448408018867924\n",
      "Average Precision for UPS1: 0.9174528301886793\n",
      "Average Recall for UPS1: 0.8484254498714654\n",
      "Average F-score for UPS1: 0.8345543123867221\n",
      "\n",
      "N-LL: -24040.2678019111\n",
      "Average Accuracy for UPS0: 0.9104634831460674\n",
      "Average Precision for UPS0: 0.8455056179775281\n",
      "Average Recall for UPS0: 0.9523117386489479\n",
      "Average F-score for UPS0: 0.8227289822930812\n",
      "\n",
      "Average Accuracy for UPS1: 0.8516725352112676\n",
      "Average Precision for UPS1: 0.9330985915492958\n",
      "Average Recall for UPS1: 0.8585691823899371\n",
      "Average F-score for UPS1: 0.8560082563396358\n",
      "\n",
      "N-LL: -24228.9579569994\n",
      "Average Accuracy for UPS0: 0.8838644524236985\n",
      "Average Precision for UPS0: 0.8105924596050269\n",
      "Average Recall for UPS0: 0.9503460686600221\n",
      "Average F-score for UPS0: 0.7882032764540763\n",
      "\n",
      "Average Accuracy for UPS1: 0.8280086848635235\n",
      "Average Precision for UPS1: 0.9292803970223326\n",
      "Average Recall for UPS1: 0.8311582109479304\n",
      "Average F-score for UPS1: 0.8332230537851162\n",
      "\n",
      "N-LL: -24025.763511120203\n",
      "Average Accuracy for UPS0: 0.8777826086956522\n",
      "Average Precision for UPS0: 0.7921739130434783\n",
      "Average Recall for UPS0: 0.9552003293084522\n",
      "Average F-score for UPS0: 0.7728828254259821\n",
      "\n",
      "Average Accuracy for UPS1: 0.8085064935064936\n",
      "Average Precision for UPS1: 0.9363636363636364\n",
      "Average Recall for UPS1: 0.8068481276005547\n",
      "Average F-score for UPS1: 0.8249539724822754\n",
      "\n",
      "N-LL: -24049.419203753532\n",
      "Average Accuracy for UPS0: 0.8737024221453287\n",
      "Average Precision for UPS0: 0.7854671280276817\n",
      "Average Recall for UPS0: 0.9520374449339206\n",
      "Average F-score for UPS0: 0.7650371882825501\n",
      "\n",
      "Average Accuracy for UPS1: 0.7984293193717278\n",
      "Average Precision for UPS1: 0.9319371727748691\n",
      "Average Recall for UPS1: 0.7955758426966293\n",
      "Average F-score for UPS1: 0.814301599841483\n",
      "\n",
      "Average Accuracy on node UPS: 0.8590718511557466\n",
      "Average Precision on node UPS: 0.8733314443221817\n",
      "Average Recall on node UPS: 0.8918209839937619\n",
      "Average F-score on node UPS: 0.8148125290500458\n"
     ]
    }
   ],
   "source": [
    "popSize = 4000\n",
    "nlls = []\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "fs = []\n",
    "df = pd.read_csv(os.getcwd() + \"/\" + trainFileName)\n",
    "kfold = KFold(n_splits=10)\n",
    "for trainDs, testDs in kfold.split(df[0:popSize]):\n",
    "    ds.read_pandas_dataframe(df.iloc[trainDs])\n",
    "    matching = ds.match_network(net)\n",
    "    em = pysmile.learning.EM()\n",
    "    em.set_randomize_parameters(True)\n",
    "    em.set_seed(10)\n",
    "    #eraseDefinitions(net)\n",
    "    #eraseTemporalDefinitions(net)\n",
    "    res = em.learn(ds, net, matching)\n",
    "    nlls.append(em.get_last_score())\n",
    "    print(\"N-LL: \" + str(em.get_last_score()))\n",
    "    \n",
    "    nodeName = NodeNames.UPS.value\n",
    "    #out = testPerf(net, nodeName, testDs = partializeEvidence(df.iloc[testDs], evidenceNodes, net.get_slice_count()))\n",
    "    out = testPerf(net, nodeName, testDs = df.iloc[testDs])\n",
    "    accs.append(out['A'])\n",
    "    precs.append(out['P'])\n",
    "    recs.append(out['R'])\n",
    "    fs.append(out['F'])\n",
    "\n",
    "print(\"Average Accuracy on node \" + nodeName + \": \" + str(np.average(accs)))\n",
    "print(\"Average Precision on node \" + nodeName + \": \" + str(np.average(precs)))\n",
    "print(\"Average Recall on node \" + nodeName + \": \" + str(np.average(recs)))\n",
    "print(\"Average F-score on node \" + nodeName + \": \" + str(np.average(fs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
