{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "#import pgmpy.models\n",
    "#import pgmpy.inference\n",
    "import numpy as np\n",
    "import pysmile\n",
    "import pysmile_license\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "numSlices = 24\n",
    "\n",
    "class NodeNames(Enum):\n",
    "    MITM = \"MITM\"\n",
    "    SRM = \"SRM\"\n",
    "    UC = \"UC\"\n",
    "    UPS = \"UPS\"\n",
    "    IMD = \"IMD\"\n",
    "    MC = \"MC\"\n",
    "    CC = \"CC\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySmile Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "#evidenceNodes = [NodeNames.IMD.value, NodeNames.MC.value, NodeNames.CC.value]\n",
    "evidenceNodes = []\n",
    "\n",
    "trainFileName = \"outTrain4.csv\"\n",
    "testFileName = \"outTest4.csv\"\n",
    "networkFileName = \"DBN-MITM.xdsl\"\n",
    "\n",
    "classNodesHandles = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node_info(net, node_handle):\n",
    "    print(\"Node id/name: \" + net.get_node_id(node_handle) + \"/\" +\n",
    "    net.get_node_name(node_handle))\n",
    "    print(\" Outcomes: \" + \" \".join(net.get_outcome_ids(node_handle)))\n",
    "    parent_ids = net.get_parent_ids(node_handle)\n",
    "    if len(parent_ids) > 0:\n",
    "        print(\" Parents: \" + \" \".join(parent_ids))\n",
    "    child_ids = net.get_child_ids(node_handle)\n",
    "    if len(child_ids) > 0:\n",
    "        print(\" Children: \" + \" \".join(child_ids))\n",
    "    print_cpt_matrix(net, node_handle)\n",
    "    \n",
    "def print_cpt_matrix(net, node_handle):\n",
    "    cpt = net.get_node_definition(node_handle)\n",
    "    parents = net.get_parents(node_handle)\n",
    "    dim_count = 1 + len(parents)\n",
    "    dim_sizes = [0] * dim_count\n",
    "    for i in range(0, dim_count - 1):\n",
    "        dim_sizes[i] = net.get_outcome_count(parents[i])\n",
    "    dim_sizes[len(dim_sizes) - 1] = net.get_outcome_count(node_handle)\n",
    "    coords = [0] * dim_count\n",
    "    for elem_idx in range(0, len(cpt)):\n",
    "        index_to_coords(elem_idx, dim_sizes, coords)\n",
    "        outcome = net.get_outcome_id(node_handle, coords[dim_count - 1])\n",
    "        out_str = \" P(\" + outcome\n",
    "        if dim_count > 1:\n",
    "            out_str += \" | \"\n",
    "            for parent_idx in range(0, len(parents)):\n",
    "                if parent_idx > 0:\n",
    "                    out_str += \",\"\n",
    "                parent_handle = parents[parent_idx]\n",
    "                out_str += net.get_node_id(parent_handle) + \"=\" + \\\n",
    "                net.get_outcome_id(parent_handle, coords[parent_idx])\n",
    "        prob = cpt[elem_idx]\n",
    "        out_str += \")=\" + str(prob)\n",
    "        print(out_str)\n",
    "    \n",
    "        \n",
    "def index_to_coords(index, dim_sizes, coords):\n",
    "    prod = 1\n",
    "    for i in range(len(dim_sizes) - 1, -1, -1):\n",
    "        coords[i] = int(index / prod) % dim_sizes[i]\n",
    "        prod *= dim_sizes[i]\n",
    "\n",
    "def pint_time_cpt_marix(net, nodeHandle):\n",
    "    timeCPT = net.get_node_temporal_definition(nodeHandle, 1)\n",
    "    \n",
    "        \n",
    "def plot_time_CPT(net, nodeHandle):    \n",
    "    cpt = net.get_node_temporal_definition(nodeHandle, 1)\n",
    "    print(len(cpt))\n",
    "    print(\"###\")\n",
    "    \n",
    "def print_net_info(net, unrolled = True):\n",
    "    for n in net.get_all_nodes():\n",
    "        print_node_info(net, n)\n",
    "        if not unrolled and net.get_node_id(n) == NodeNames.UPS.value:\n",
    "            plot_time_CPT(net, n)\n",
    "            \n",
    "def calc_stat(confMatrix, outcome, type='P'):\n",
    "    TP = confMatrix[outcome][outcome]\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(0, len(confMatrix)):\n",
    "        for j in range(0, len(confMatrix[i])):\n",
    "            if i == outcome and j != outcome:\n",
    "                FP += confMatrix[i][j]\n",
    "            if j == outcome and i != outcome:\n",
    "                FN += confMatrix[i][j]\n",
    "            if i != outcome and j != outcome:\n",
    "                TN += confMatrix[i][j] \n",
    "                \n",
    "    if type == 'P':\n",
    "        if TP + FP == 0: return float('nan')\n",
    "        return TP / (TP + FP)\n",
    "    if type == 'A':\n",
    "        if TP + TN + FP + FN == 0: return float('nan')\n",
    "        return  (TP+TN) / (TP + TN + FP + FN)\n",
    "    if type == 'R':\n",
    "        if TP + FN == 0: return float('nan')\n",
    "        return TP / (TP + FN)\n",
    "    if type == 'F':\n",
    "        if (2*TP)+FP+FN == 0: return float('nan')\n",
    "        return (2*TP)/((2*TP)+FP+FN)        \n",
    "\n",
    "def print_validator_results(net, originalSliceCount, validator, nodeId): \n",
    "    nodeHandle = classNodesHandles[nodeId]\n",
    "    outcomeCount = net.get_outcome_count(nodeHandle)\n",
    "    accMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    precMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    recMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    fMtrx = np.zeros((outcomeCount, 2, originalSliceCount))\n",
    "    \n",
    "    \n",
    "    for slice in range(1, originalSliceCount):\n",
    "        if DEBUG: print(\"### Slice \" + str(slice) + \" ###\")\n",
    "        nodeHandle = classNodesHandles[nodeId + \"_\" + str(slice)]\n",
    "        cm = validator.get_confusion_matrix(nodeHandle)\n",
    "        for i in range(0, outcomeCount):\n",
    "            acc = calc_stat(cm, i, 'A')\n",
    "            # If the calculated statistic is NaN assign 0 (see next line)\n",
    "            accMtrx[i][0][slice] = acc if not math.isnan(acc) else 0\n",
    "            # If the calculated statistic is NaN assign weight 0 to it\n",
    "            # otherwise use the number of elements belonging to that class as weight\n",
    "            accMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(acc) else 0\n",
    "            prec = calc_stat(cm, i, 'P')\n",
    "            precMtrx[i][0][slice] = prec if not math.isnan(prec) else 0\n",
    "            precMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(prec) else 0\n",
    "            rec = calc_stat(cm, i, 'R')\n",
    "            recMtrx[i][0][slice] = rec if not math.isnan(rec) else 0\n",
    "            recMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(rec) else 0\n",
    "            f = calc_stat(cm, i, 'F')\n",
    "            fMtrx[i][0][slice] = f if not math.isnan(f) else 0\n",
    "            fMtrx[i][1][slice] = np.sum(cm[:][i]) if not math.isnan(f) else 0\n",
    "            \n",
    "            if DEBUG:\n",
    "                print(\"Accuracy for \" + nodeId + str(i) + \": \" + str(acc))\n",
    "                print(\"Precision for \" + nodeId + str(i) + \": \" + str(prec))\n",
    "                print(\"Recall for \" + nodeId + str(i) + \": \" + str(rec))    \n",
    "        if DEBUG:    \n",
    "            print(\"** Confusion Matrix **\")\n",
    "            for i in range(0, outcomeCount):\n",
    "                print(cm[i])\n",
    "            print(\"\")\n",
    "    \n",
    "    # Calculates weighted averages (each statistic of each timeslice is weighted\n",
    "    # by the number of elements nelonging to that class in that timeslice)\n",
    "    avgAccOut = np.zeros(outcomeCount)\n",
    "    avgPrecOut = np.zeros(outcomeCount)\n",
    "    avgRecOut = np.zeros(outcomeCount)\n",
    "    avgFOut = np.zeros(outcomeCount)\n",
    "    for i in range(0, outcomeCount):\n",
    "        avgAccOut[i] = np.sum(accMtrx[i][0]*accMtrx[i][1])/np.sum(accMtrx[i][1])\n",
    "        avgPrecOut[i] = np.sum(precMtrx[i][0]*precMtrx[i][1])/np.sum(precMtrx[i][1])\n",
    "        avgRecOut[i] = np.sum(recMtrx[i][0]*recMtrx[i][1])/np.sum(recMtrx[i][1])\n",
    "        avgFOut[i] = np.sum(fMtrx[i][0]*fMtrx[i][1])/np.sum(fMtrx[i][1])\n",
    "        \n",
    "        \n",
    "        print(\"Average Accuracy for \" + nodeId + str(i) + \": \" + str(avgAccOut[i]))\n",
    "        print(\"Average Precision for \" + nodeId + str(i) + \": \" + str(avgPrecOut[i]))\n",
    "        print(\"Average Recall for \" + nodeId + str(i) + \": \" + str(avgRecOut[i]))\n",
    "        print(\"Average F-score for \" + nodeId + str(i) + \": \" + str(avgFOut[i]))\n",
    "        print(\"\") \n",
    "    \n",
    "    out = {}\n",
    "    out['A'] = np.average(avgAccOut)\n",
    "    out['P'] = np.average(avgPrecOut) \n",
    "    out['R'] = np.average(avgRecOut) \n",
    "    out['F'] = np.average(avgFOut)\n",
    "    return out\n",
    "        \n",
    "            \n",
    "def eraseDefinitions(net):\n",
    "    nodes = net.get_all_node_ids()\n",
    "    for node in nodes:\n",
    "        cpt = net.get_node_definition(node)\n",
    "        numOutcomes = net.get_outcome_count(node)\n",
    "        p = 1 / numOutcomes\n",
    "        for i in range(0, len(cpt)):\n",
    "            cpt[i] = p\n",
    "        net.set_node_definition(node, cpt)\n",
    "\n",
    "def eraseTemporalDefinitions(net):\n",
    "    nodes = net.get_all_node_ids()\n",
    "    for node in nodes:\n",
    "        try:\n",
    "            cpt = net.get_node_temporal_definition(node, 1)\n",
    "            numOutcomes = net.get_outcome_count(node)\n",
    "            p = 1 / numOutcomes\n",
    "            for i in range(0, len(cpt)):\n",
    "                cpt[i] = p\n",
    "            net.set_node_temporal_definition(node, 1, cpt)  \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def partializeEvidence(df, evidenceNodes, sliceCount):\n",
    "    for evidenceNode in evidenceNodes:\n",
    "        for slice in range(1, sliceCount):\n",
    "            if 1 <= slice <= 20:\n",
    "                colName = evidenceNode + \"_\" + str(slice)\n",
    "                df = df.drop(colName, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and read the DBN from file\n",
    "net = pysmile.Network()\n",
    "ds = pysmile.learning.DataSet()\n",
    "\n",
    "net.read_file(os.getcwd() + \"/../../../Genie-DBN/\" + networkFileName)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erase CPTs before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "eraseDefinitions(net)\n",
    "eraseTemporalDefinitions(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traininig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-LL: -183900.29110254004\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.getcwd() + \"/\" + trainFileName)\n",
    "ds.read_pandas_dataframe(df)\n",
    "\n",
    "matching = ds.match_network(net)\n",
    "em = pysmile.learning.EM()\n",
    "# Small data variatons correspond to big changes\n",
    "em.set_relevance(0)\n",
    "res = em.learn(ds, net, matching)\n",
    "\n",
    "print(\"N-LL: \" + str(em.get_last_score()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPerf(net, nodeName, testDs = None):\n",
    "    if testDs is None:\n",
    "        testDs = pd.read_csv(os.getcwd() + \"/\" + testFileName)\n",
    "        \n",
    "    ds.read_pandas_dataframe(testDs)\n",
    "    \n",
    "    unrolledNet = net.unroll().unrolled     \n",
    "    matching = ds.match_network(unrolledNet)\n",
    "    validator = pysmile.learning.Validator(unrolledNet, ds, matching)\n",
    "    # Set class nodes (those that will not be considered as evidence nodes)\n",
    "    for elem in NodeNames.__members__:\n",
    "        if elem not in evidenceNodes:\n",
    "            classNodesHandles[elem] = unrolledNet.get_node(elem)\n",
    "            validator.add_class_node(classNodesHandles[elem])\n",
    "            for slice in range(1, net.get_slice_count()):\n",
    "                elemCat = elem + \"_\" + str(slice)\n",
    "                classNodesHandles[elemCat] = unrolledNet.get_node(elemCat)\n",
    "                validator.add_class_node(classNodesHandles[elemCat])\n",
    "    # Test the predctions on the class nodes            \n",
    "    validator.test()\n",
    "    #print(validator.get_result_dataset())\n",
    "    if DEBUG:\n",
    "        print_net_info(unrolledNet, unrolled=True)\n",
    "    \n",
    "    return print_validator_results(unrolledNet, net.get_slice_count(), validator, nodeName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "4000\n",
      "Duplicate rows: 4003\n"
     ]
    }
   ],
   "source": [
    "def compare_files(trainFileName, testFileName):\n",
    "    dfTrain = pd.read_csv(os.getcwd() + \"/\" + trainFileName)\n",
    "    dfTest = pd.read_csv(os.getcwd() + \"/\" + testFileName)\n",
    "    \n",
    "    dfOut = pd.concat([dfTrain, dfTest]).drop_duplicates(keep=\"first\")\n",
    "    dups  = dfTrain.shape[0] + dfTest.shape[0] - dfOut.shape[0]\n",
    "    print(dfTrain.shape[0])\n",
    "    print(dfTest.shape[0])\n",
    "    print(\"Duplicate rows: \" + str(dups))\n",
    "    \n",
    "compare_files(\"outTrain4.csv\", \"outTrain4.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test learning capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-LL: -165861.66481208976\n",
      "Average Accuracy for UPS0: 0.8723580941810687\n",
      "Average Precision for UPS0: 0.8558744252417948\n",
      "Average Recall for UPS0: 0.9151528343831048\n",
      "Average F-score for UPS0: 0.8150308128760482\n",
      "\n",
      "Average Accuracy for UPS1: 0.7856172183419375\n",
      "Average Precision for UPS1: 0.8171879744913453\n",
      "Average Recall for UPS1: 0.7911677814938686\n",
      "Average F-score for UPS1: 0.7137140566674347\n",
      "\n",
      "N-LL: -165871.51980629924\n",
      "Average Accuracy for UPS0: 0.8610503273945485\n",
      "Average Precision for UPS0: 0.8317344297243795\n",
      "Average Recall for UPS0: 0.9219434273160014\n",
      "Average F-score for UPS0: 0.7957411892921568\n",
      "\n",
      "Average Accuracy for UPS1: 0.7591435872073855\n",
      "Average Precision for UPS1: 0.8226178700956149\n",
      "Average Recall for UPS1: 0.7519589178356715\n",
      "Average F-score for UPS1: 0.6947314258056108\n",
      "\n",
      "N-LL: -165647.2211469137\n",
      "Average Accuracy for UPS0: 0.8573521820741437\n",
      "Average Precision for UPS0: 0.8390427029563585\n",
      "Average Recall for UPS0: 0.9075307606263981\n",
      "Average F-score for UPS0: 0.7957817362753755\n",
      "\n",
      "Average Accuracy for UPS1: 0.7651847521047709\n",
      "Average Precision for UPS1: 0.8016838166510758\n",
      "Average Recall for UPS1: 0.7615490081680281\n",
      "Average F-score for UPS1: 0.6842101472888832\n",
      "\n",
      "N-LL: -165332.7969163381\n",
      "Average Accuracy for UPS0: 0.8617983705109951\n",
      "Average Precision for UPS0: 0.8473342825502294\n",
      "Average Recall for UPS0: 0.9073263629574309\n",
      "Average F-score for UPS0: 0.8033575733863244\n",
      "\n",
      "Average Accuracy for UPS1: 0.7757159194876486\n",
      "Average Precision for UPS1: 0.8035986581274779\n",
      "Average Recall for UPS1: 0.7769307400379507\n",
      "Average F-score for UPS1: 0.694387340611827\n",
      "\n",
      "N-LL: -165055.0332142319\n",
      "Average Accuracy for UPS0: 0.8614517802969823\n",
      "Average Precision for UPS0: 0.8534248762573846\n",
      "Average Recall for UPS0: 0.9071529466791394\n",
      "Average F-score for UPS0: 0.808692709697175\n",
      "\n",
      "Average Accuracy for UPS1: 0.7886507341923884\n",
      "Average Precision for UPS1: 0.8037159124962541\n",
      "Average Recall for UPS1: 0.7962192393736018\n",
      "Average F-score for UPS1: 0.7031045001705075\n",
      "\n",
      "N-LL: -165176.5229457118\n",
      "Average Accuracy for UPS0: 0.8713712192079826\n",
      "Average Precision for UPS0: 0.8481446835048332\n",
      "Average Recall for UPS0: 0.9210551470588235\n",
      "Average F-score for UPS0: 0.8105475104675903\n",
      "\n",
      "Average Accuracy for UPS1: 0.7774717514124294\n",
      "Average Precision for UPS1: 0.8242310106716887\n",
      "Average Recall for UPS1: 0.7797277227722772\n",
      "Average F-score for UPS1: 0.7127386508184788\n",
      "\n",
      "N-LL: -165391.64583470146\n",
      "Average Accuracy for UPS0: 0.8626221264367816\n",
      "Average Precision for UPS0: 0.8529693486590039\n",
      "Average Recall for UPS0: 0.9067672655811342\n",
      "Average F-score for UPS0: 0.808118784829641\n",
      "\n",
      "Average Accuracy for UPS1: 0.7849325539568345\n",
      "Average Precision for UPS1: 0.8030575539568345\n",
      "Average Recall for UPS1: 0.791486562150056\n",
      "Average F-score for UPS1: 0.7007358939958152\n",
      "\n",
      "N-LL: -165267.61828455678\n",
      "Average Accuracy for UPS0: 0.8659731861198738\n",
      "Average Precision for UPS0: 0.8506309148264984\n",
      "Average Recall for UPS0: 0.9116646578898573\n",
      "Average F-score for UPS0: 0.808879600078156\n",
      "\n",
      "Average Accuracy for UPS1: 0.7839662576687115\n",
      "Average Precision for UPS1: 0.8138036809815951\n",
      "Average Recall for UPS1: 0.7837683754240481\n",
      "Average F-score for UPS1: 0.7066381081839331\n",
      "\n",
      "N-LL: -165804.05836375413\n",
      "Average Accuracy for UPS0: 0.8749055727554179\n",
      "Average Precision for UPS0: 0.8456656346749226\n",
      "Average Recall for UPS0: 0.9246883580450302\n",
      "Average F-score for UPS0: 0.8099257203143161\n",
      "\n",
      "Average Accuracy for UPS1: 0.7688248407643312\n",
      "Average Precision for UPS1: 0.8289808917197452\n",
      "Average Recall for UPS1: 0.7693747598924318\n",
      "Average F-score for UPS1: 0.7119110962037986\n",
      "\n",
      "N-LL: -165702.2174987635\n",
      "Average Accuracy for UPS0: 0.8612201091317605\n",
      "Average Precision for UPS0: 0.8528326111378591\n",
      "Average Recall for UPS0: 0.9013520888219797\n",
      "Average F-score for UPS0: 0.8056290429383169\n",
      "\n",
      "Average Accuracy for UPS1: 0.780865983971505\n",
      "Average Precision for UPS1: 0.7963787474027901\n",
      "Average Recall for UPS1: 0.7851369735370853\n",
      "Average F-score for UPS1: 0.6930390381883782\n",
      "\n",
      "Average Accuracy on node UPS: 0.821023828360875\n",
      "Average Precision on node UPS: 0.8296455013063844\n",
      "Average Recall on node UPS: 0.8455976965021959\n",
      "Average F-score on node UPS: 0.7538457469044884\n"
     ]
    }
   ],
   "source": [
    "popSize = 4000\n",
    "nlls = []\n",
    "accs = []\n",
    "precs = []\n",
    "recs = []\n",
    "fs = []\n",
    "df = pd.read_csv(os.getcwd() + \"/\" + trainFileName)\n",
    "kfold = KFold(n_splits=10)\n",
    "for trainDs, testDs in kfold.split(df[0:popSize]):\n",
    "    ds.read_pandas_dataframe(df.iloc[trainDs])\n",
    "    matching = ds.match_network(net)\n",
    "    em = pysmile.learning.EM()\n",
    "    em.set_randomize_parameters(True)\n",
    "    em.set_seed(10)\n",
    "    #eraseDefinitions(net)\n",
    "    #eraseTemporalDefinitions(net)\n",
    "    res = em.learn(ds, net, matching)\n",
    "    nlls.append(em.get_last_score())\n",
    "    print(\"N-LL: \" + str(em.get_last_score()))\n",
    "    \n",
    "    nodeName = NodeNames.UPS.value\n",
    "    out = testPerf(net, nodeName, testDs = partializeEvidence(df.iloc[testDs], evidenceNodes, net.get_slice_count()))\n",
    "    accs.append(out['A'])\n",
    "    precs.append(out['P'])\n",
    "    recs.append(out['R'])\n",
    "    fs.append(out['F'])\n",
    "\n",
    "print(\"Average Accuracy on node \" + nodeName + \": \" + str(np.average(accs)))\n",
    "print(\"Average Precision on node \" + nodeName + \": \" + str(np.average(precs)))\n",
    "print(\"Average Recall on node \" + nodeName + \": \" + str(np.average(recs)))\n",
    "print(\"Average F-score on node \" + nodeName + \": \" + str(np.average(fs)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 states for SRM ecc... (complete evidence)\n",
    "Average Accuracy on node UPS: 0.7691188647062448\n",
    "Average Precision on node UPS: 0.7580515072136919\n",
    "Average Recall on node UPS: 0.7915767405552715\n",
    "Average F-score on node UPS: 0.7707809782488163\n",
    "\n",
    "# 4 states for SRM ecc... (evidence just on odd timeslices)\n",
    "Average Accuracy on node UPS: 0.7677149087383711\n",
    "Average Precision on node UPS: 0.7589832019963916\n",
    "Average Recall on node UPS: 0.7915573192540584\n",
    "Average F-score on node UPS: 0.7708178971171644\n",
    "\n",
    "# 2 states for all the nodes (complete evidence)\n",
    "Average Accuracy on node UPS: 0.8066597430345286\n",
    "Average Precision on node UPS: 0.8114346737200531\n",
    "Average Recall on node UPS: 0.7885880802092043\n",
    "Average F-score on node UPS: 0.790012708581611\n",
    "\n",
    "# 2 states for all the nodes (evidence just on odd timeslices)\n",
    "Average Accuracy on node UPS: 0.8116308516361525\n",
    "Average Precision on node UPS: 0.8152992459442799\n",
    "Average Recall on node UPS: 0.7921912497608095\n",
    "Average F-score on node UPS: 0.7927440542420979"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
